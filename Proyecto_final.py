# -*- coding: utf-8 -*-
"""Copia de Proyecto Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nSbkmTo_gKWOszGsiNLgO2jaIutqp8EZ

# PROYECTO #

## Integrantes: ##
* Lina Maria Ayure Lara - 20221678039
* Daniel Alejandro Rojas - 20221678049

# INTRODUCCIÓN

##La retención de empleados representa un desafío crítico para las organizaciones debido a su impacto en costos, productividad y ganancias. La pérdida de empleados experimentados genera gastos significativos en la contratación y capacitación de reemplazos, además de afectar la cohesión del equipo y la calidad del trabajo.

##Este proyecto se enfoca en analizar los factores que influyen en la retención de empleados y desarrollar un modelo de predicción para estimar la probabilidad de desgaste de los empleados. La meta es proporcionar recomendaciones que ayuden a las organizaciones a mejorar la retención de empleados, reduciendo costos y optimizando su rendimiento global.

# DESARROLLO DEL PROYECTO DE ANALISIS

# IMPORTACIONES #

### Para iniciar el proyecto, que implica un análisis exhaustivo de los datos contenidos en los conjuntos de datos (Dataset), es fundamental llevar a cabo la configuración inicial del entorno de trabajo. Esto implica la importación de las bibliotecas necesarias que serán utilizadas en el proceso de análisis de datos. ###
"""
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

"""# CARGAR DATAFRAMES #

### Posteriormente, procedemos a cargar mediante pandas los archivo en formato .csv, dado que estos archivos albergan los datos necesarios para llevar a cabo el análisis correspondiente.###
"""

df_empleados = pd.read_csv("DataSetEmpleado.csv")
df_empleados

"""# DATA CLEANING (Limpieza de Datos) #

## A continuación, después de importar los archivos en formato .csv y antes de proceder con el análisis, se lleva a cabo una exhaustiva limpieza de los datos. El objetivo principal de esta etapa es identificar y abordar posibles valores nulos o atípicos que puedan estar presentes en los datos. Si se encuentran tales datos (en caso de que existan), se emprende un proceso para resolverlos y así garantizar una mayor precisión en los resultados finales. ##
"""

print(df_empleados.info())
print(df_empleados.describe())

"""## Ahora, vamos a verificar la presencia de valores nulos en nuestro dataframe para asegurarnos de la integridad de los datos. ##

"""

print(df_empleados.isnull().all())

"""## Realizaremos una comprobación para asegurarnos de que no existan datos duplicados en las columnas de nuestros dataframes.##"""

print(df_empleados.duplicated().value_counts())

"""## Seleccionamos las columnas más relevantes para este caso, de manera que podamos simplificar un poco las caracteristicas más importantes. ##"""

columnas_seleccionadas = ['Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome', 'Education', 'EducationField', 'TotalWorkingYears', 'WorkLifeBalance']
df_filtrado = df_empleados[columnas_seleccionadas]
df_filtrado

"""## Graficamos los datos de las columnas seleccionadas ##"""

plt.hist(df_filtrado['Age'], bins=10, edgecolor='black')
plt.title('Histograma de {}'.format('Edad'))
plt.xlabel('Edad')
plt.ylabel('Frecuencia')
plt.show()

plt.hist(df_filtrado['Attrition'], bins=10, edgecolor='black')
plt.title('Histograma de {}'.format('Desgaste'))
plt.xlabel('Desgaste')
plt.ylabel('Frecuencia')
plt.show()

plt.hist(df_filtrado['BusinessTravel'], bins=10, edgecolor='black')
plt.title('Histograma de {}'.format('Viajes de Negocios'))
plt.xlabel('Viajes de Negocios')
plt.ylabel('Frecuencia')
plt.show()

plt.hist(df_filtrado['DailyRate'], bins=10, edgecolor='black')
plt.title('Histograma de {}'.format('Tarifa Diaria'))
plt.xlabel('Tarifa Diaria')
plt.ylabel('Frecuencia')
plt.show()

plt.hist(df_filtrado['Department'], bins=10, edgecolor='black')
plt.title('Histograma de {}'.format('Departamento'))
plt.xlabel('Departamento')
plt.ylabel('Frecuencia')
plt.show()

plt.hist(df_filtrado['DistanceFromHome'], bins=10, edgecolor='black')
plt.title('Histograma de {}'.format('Distancia desde casa (KM)'))
plt.xlabel('Distancia desde casa')
plt.ylabel('Frecuencia')
plt.show()

plt.hist(df_filtrado['Education'], bins=10, edgecolor='black')
plt.title('Histograma de {}'.format('Educación (Nivel)'))
plt.xlabel('Educacion')
plt.ylabel('Frecuencia')
plt.show()

plt.figure(figsize=(10, 5))
plt.hist(df_filtrado['EducationField'], bins=20, edgecolor='black')
plt.title('Histograma de {}'.format('Campo de Educación'))
plt.xlabel('Campo de Educación')
plt.ylabel('Frecuencia')
plt.show()

plt.hist(df_filtrado['TotalWorkingYears'], bins=10, edgecolor='black')
plt.title('Histograma de {}'.format('Años de Trabajo Totales'))
plt.xlabel('Años de Trabajo Totales')
plt.ylabel('Frecuencia')
plt.show()

plt.hist(df_filtrado['WorkLifeBalance'], bins=10, edgecolor='black')
plt.title('Histograma de {}'.format('Equilibrio entre trabajo y vida personal'))
plt.xlabel('Equilibrio entre trabajo y vida personal')
plt.ylabel('Frecuencia')
plt.show()

"""# DATA ANALYSIS (Analisis de datos) #

## Una vez que hemos realizado la limplieza de los datos, necesitamos ver la correlación de los diferentes campos que hemos elegido para el caso ##
"""

df_filtrado.corr(method = 'spearman')

"""## Ahora representamos graficamente los resultados para tener una mejor visualización. ##"""

plt.figure(figsize=(15,10))
sns.heatmap(df_filtrado.corr(), annot=True, linewidths=.5, vmin = -1, vmax = 1, fmt = '.2g')

"""##Identificación de columnas con datos categóricos o identificadores únicos. Esto es útil para determinar si una columna es candidata para ser utilizada como un identificador único o para identificar posibles columnas con categorías que necesitan ser codificadas de manera adecuada."""

print(df_filtrado.nunique())

"""## Ahora procedemos a convertir esos valores categoricos a valores binarios ##"""

df_filtrado = pd.get_dummies(df_filtrado, columns=['Attrition', 'BusinessTravel', 'Department', 'EducationField'])
df_filtrado

"""## Las columnas finales a usar seran estas. ##"""

df_filtrado.columns

"""# ALGORITMO DE REGRESIÓN LINEAL #"""

# Dividir el conjunto de datos en características (X) y variable objetivo (y)
X = df_filtrado.drop(['Attrition_Yes','Attrition_No'], axis=1)
print("X:", X)
y = df_filtrado['Attrition_Yes']

nuevos_datos = {
    'Age': [28],
    'DailyRate': [600],
    'DistanceFromHome': [10],
    'Education': [3],
    'TotalWorkingYears': [5],
    'WorkLifeBalance': [2],
    'BusinessTravel_Non-Travel': [0],
    'BusinessTravel_Travel_Frequently': [1],
    'BusinessTravel_Travel_Rarely': [0],
    'Department_Human Resources': [0],
    'Department_Research & Development': [1],
    'Department_Sales': [0],
    'EducationField_Human Resources': [0],
    'EducationField_Life Sciences': [1],
    'EducationField_Marketing': [0],
    'EducationField_Medical': [0],
    'EducationField_Other': [0],
    'EducationField_Technical Degree': [0]
}
nuevos_datos_df = pd.DataFrame(nuevos_datos)
# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Inicializar el modelo de regresión logística
model = LogisticRegression()

# Entrenar el modelo
model.fit(X_train, y_train)

# Hacer predicciones en el conjunto de prueba
y_pred = model.predict(X_test)

# Evaluar la precisión del modelo
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Utilizar el modelo entrenado para predecir la probabilidad de atrición
probabilidades_predichas = model.predict_proba(nuevos_datos_df)[:, 1]

# Imprimir la probabilidad predicha
print(f"Probabilidad de Desgaste: {probabilidades_predichas[0] * 100:.2f}%")

print(f'Accuracy: {accuracy}%')
print(f'Confusion Matrix:\n{conf_matrix}')

"""## Graficamos la matriz de confusión ##"""

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", annot_kws={"size": 16})
plt.title('Matriz de Confusión')
plt.xlabel('Predicción')
plt.ylabel('Real')
plt.show()